{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nTrain Your Own Neural Network Potential\n=======================================\n\nThis example shows how to use TorchANI train your own neural network potential.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin with, let's first import the modules we will use:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport ignite\nimport torchani\nimport timeit\nimport tensorboardX\nimport os\nimport ignite.contrib.handlers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's setup training hyperparameters. Note that here for our demo purpose\n, we set both training set and validation set the ``ani_gdb_s01.h5`` in\nTorchANI's repository. This allows this program to finish very quick, because\nthat dataset is very small. But this is wrong and should be avoided for any\nserious training. These paths assumes the user run this script under the\n``examples`` directory of TorchANI's repository. If you download this script,\nyou should manually set the path of these files in your system before this\nscript can run successfully.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# training and validation set\ntry:\n    path = os.path.dirname(os.path.realpath(__file__))\nexcept NameError:\n    path = os.getcwd()\ntraining_path = os.path.join(path, '../dataset/ani_gdb_s01.h5')\nvalidation_path = os.path.join(path, '../dataset/ani_gdb_s01.h5')\n\n# checkpoint file to save model when validation RMSE improves\nmodel_checkpoint = 'model.pt'\n\n# max epochs to run the training\nmax_epochs = 20\n\n# Compute training RMSE every this steps. Since the training set is usually\n# huge and the loss funcition does not directly gives us RMSE, we need to\n# check the training RMSE to see overfitting.\ntraining_rmse_every = 5\n\n# device to run the training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# batch size\nbatch_size = 1024\n\n# log directory for tensorboardX\nlog = 'runs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's read our constants and self energies from constant files and\nconstruct AEV computer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "const_file = os.path.join(path, '../torchani/resources/ani-1x_8x/rHCNO-5.2R_16-3.5A_a4-8.params')  # noqa: E501\nsae_file = os.path.join(path, '../torchani/resources/ani-1x_8x/sae_linfit.dat')  # noqa: E501\nconsts = torchani.neurochem.Constants(const_file)\naev_computer = torchani.AEVComputer(**consts)\nenergy_shifter = torchani.neurochem.load_sae(sae_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define atomic neural networks. Here in this demo, we use the same\nsize of neural network for all atom types, but this is not necessary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def atomic():\n    model = torch.nn.Sequential(\n        torch.nn.Linear(384, 128),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(128, 128),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(128, 64),\n        torch.nn.CELU(0.1),\n        torch.nn.Linear(64, 1)\n    )\n    return model\n\n\nnn = torchani.ANIModel([atomic() for _ in range(4)])\nprint(nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If checkpoint from previous training exists, then load it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(model_checkpoint):\n    nn.load_state_dict(torch.load(model_checkpoint))\nelse:\n    torch.save(nn.state_dict(), model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now create a pipeline of AEV Computer --> Neural Networks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Sequential(aev_computer, nn).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now setup tensorboardX.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "writer = tensorboardX.SummaryWriter(log_dir=log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now load training and validation datasets into memory. Note that we need to\nsubtracting energies by the self energies of all atoms for each molecule.\nThis makes the range of energies in a reasonable range. The second argument\ndefines how to convert species as a list of string to tensor, that is, for\nall supported chemical symbols, which is correspond to ``0``, which\ncorrespond to ``1``, etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training = torchani.data.BatchedANIDataset(\n    training_path, consts.species_to_tensor, batch_size, device=device,\n    transform=[energy_shifter.subtract_from_dataset])\n\nvalidation = torchani.data.BatchedANIDataset(\n    validation_path, consts.species_to_tensor, batch_size, device=device,\n    transform=[energy_shifter.subtract_from_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When iterating the dataset, we will get pairs of input and output\n``(species_coordinates, properties)``, where ``species_coordinates`` is the\ninput and ``properties`` is the output.\n\n``species_coordinates`` is a list of species-coordinate pairs, with shape\n``(N, Na)`` and ``(N, Na, 3)``. The reason for getting this type is, when\nloading the dataset and generating minibatches, the whole dataset are\nshuffled and each minibatch contains structures of molecules with a wide\nrange of number of atoms. Molecules of different number of atoms are batched\ninto single by padding. The way padding works is: adding ghost atoms, with\nspecies 'X', and do computations as if they were normal atoms. But when\ncomputing AEVs, atoms with species `X` would be ignored. To avoid computation\nwasting on padding atoms, minibatches are further splitted into chunks. Each\nchunk contains structures of molecules of similar size, which minimize the\ntotal number of padding atoms required to add. The input list\n``species_coordinates`` contains chunks of that minibatch we are getting. The\nbatching and chunking happens automatically, so the user does not need to\nworry how to construct chunks, but the user need to compute the energies for\neach chunk and concat them into single tensor.\n\nThe output, i.e. ``properties`` is a dictionary holding each property. This\nallows us to extend TorchANI in the future to training forces and properties.\n\nWe have tools to deal with these data types at :attr:`torchani.ignite` that\nallow us to easily combine the dataset with pytorch ignite. These tools can\nbe used as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "container = torchani.ignite.Container({'energies': model})\noptimizer = torch.optim.Adam(model.parameters())\ntrainer = ignite.engine.create_supervised_trainer(\n    container, optimizer, torchani.ignite.MSELoss('energies'))\nevaluator = ignite.engine.create_supervised_evaluator(container, metrics={\n        'RMSE': torchani.ignite.RMSEMetric('energies')\n    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add a progress bar for the trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pbar = ignite.contrib.handlers.ProgressBar()\npbar.attach(trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And some event handlers to compute validation and training metrics:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def hartree2kcal(x):\n    return 627.509 * x\n\n\n@trainer.on(ignite.engine.Events.EPOCH_STARTED)\ndef validation_and_checkpoint(trainer):\n    def evaluate(dataset, name):\n        evaluator = ignite.engine.create_supervised_evaluator(\n            container,\n            metrics={\n                'RMSE': torchani.ignite.RMSEMetric('energies')\n            }\n        )\n        evaluator.run(dataset)\n        metrics = evaluator.state.metrics\n        rmse = hartree2kcal(metrics['RMSE'])\n        writer.add_scalar(name, rmse, trainer.state.epoch)\n\n    # compute validation RMSE\n    evaluate(validation, 'validation_rmse_vs_epoch')\n\n    # compute training RMSE\n    if trainer.state.epoch % training_rmse_every == 1:\n        evaluate(training, 'training_rmse_vs_epoch')\n\n    # checkpoint model\n    torch.save(nn.state_dict(), model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also some to log elapsed time:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n\n\n@trainer.on(ignite.engine.Events.EPOCH_STARTED)\ndef log_time(trainer):\n    elapsed = round(timeit.default_timer() - start, 2)\n    writer.add_scalar('time_vs_epoch', elapsed, trainer.state.epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also log the loss per iteration:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@trainer.on(ignite.engine.Events.ITERATION_COMPLETED)\ndef log_loss(trainer):\n    iteration = trainer.state.iteration\n    writer.add_scalar('loss_vs_iteration', trainer.state.output, iteration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally, we are ready to run:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.run(training, max_epochs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}